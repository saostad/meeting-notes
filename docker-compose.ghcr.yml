# Docker Compose configuration using pre-built images from GitHub Container Registry
# Replace 'YOUR_GITHUB_USERNAME' with your actual GitHub username

services:
  # CPU-only service using GHCR image (Requirements 5.1, 3.1)
  meeting-video-tool-cpu:
    image: ghcr.io/YOUR_GITHUB_USERNAME/meeting-video-tool:cpu
    container_name: meeting-video-tool-cpu
    
    # Volume mounts for input/output and model caching (Requirements 5.1, 5.2, 7.5)
    volumes:
      - ./videos:/input:ro                    # Input videos (read-only)
      - ./output:/output                      # Output files
      - model-cache:/cache                    # Persistent model cache
    
    # Environment configuration (Requirement 3.1)
    environment:
      # Required API key (Requirement 3.1)
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      
      # Model configuration (Requirement 7.2)
      - WHISPER_MODEL=${WHISPER_MODEL:-openai/whisper-large-v3-turbo}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-flash-latest}
      
      # Performance optimization settings (Requirements 7.3, 7.5)
      - SKIP_EXISTING=${SKIP_EXISTING:-false}
      - OUTPUT_DIR=/output
      - OVERLAY_CHAPTER_TITLES=${OVERLAY_CHAPTER_TITLES:-false}
      
      # Model caching configuration (Requirement 7.5)
      - HF_HOME=/cache/huggingface
      - TRANSFORMERS_CACHE=/cache/huggingface
      - HF_DATASETS_CACHE=/cache/huggingface/datasets
      - TORCH_HOME=/cache/models
    
    # Resource limits for optimal CPU performance
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    
    # Default command shows help
    command: ["python", "-m", "src.main", "--help"]

  # GPU-enabled service using GHCR image (Requirement 6.1)
  meeting-video-tool-gpu:
    image: ghcr.io/YOUR_GITHUB_USERNAME/meeting-video-tool:gpu
    container_name: meeting-video-tool-gpu
    
    # Volume mounts identical to CPU version (Requirements 5.1, 5.2)
    volumes:
      - ./videos:/input:ro
      - ./output:/output
      - model-cache:/cache
    
    # Environment configuration with GPU-specific settings (Requirements 6.1, 3.1)
    environment:
      # Required API key
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      
      # Model configuration
      - WHISPER_MODEL=${WHISPER_MODEL:-openai/whisper-large-v3-turbo}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-flash-latest}
      
      # Performance optimization settings
      - SKIP_EXISTING=${SKIP_EXISTING:-false}
      - OUTPUT_DIR=/output
      - OVERLAY_CHAPTER_TITLES=${OVERLAY_CHAPTER_TITLES:-false}
      
      # Model caching configuration
      - HF_HOME=/cache/huggingface
      - TRANSFORMERS_CACHE=/cache/huggingface
      - HF_DATASETS_CACHE=/cache/huggingface/datasets
      - TORCH_HOME=/cache/models
      
      # GPU-specific environment variables (Requirement 6.1)
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    # GPU resource allocation and limits (Requirement 6.1)
    deploy:
      resources:
        limits:
          memory: 12G
          cpus: '6.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
          memory: 6G
          cpus: '3.0'
    
    # Default command shows help
    command: ["python", "-m", "src.main", "--help"]

# Named volumes for persistent model caching (Requirement 7.5)
volumes:
  model-cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/cache