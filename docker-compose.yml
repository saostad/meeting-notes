services:
  # Single unified service for development environment (Requirements 3.1, 6.1, 6.2, 8.1, 8.4, 8.5)
  app:
    # Use pre-built image from GitHub Container Registry
    # To build locally instead, comment out 'image' and uncomment 'build: .'
    # image: ghcr.io/yourusername/meeting-video-tool:latest
    build: .
    container_name: meeting-video-tool-dev
    
    # Optimized volume mounts for Windows performance
    volumes:
      - ./workspace:/workspace:cached       # Cached mount for better performance
      - ./src:/app/src:ro,cached           # Read-only cached mount for source
      - ./videos:/videos:ro,cached         # Read-only cached mount for videos
      - ./.env:/app/.env:ro                # Mount .env file for direct access
    
    # Environment configuration (Requirements 3.1, 6.1, 6.2)
    # Variables are loaded from .env file with docker-compose variable substitution
    # The .env file is also mounted directly for the Python application to access
    environment:
      # Required API key
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      
      # Model configuration
      - WHISPER_MODEL=${WHISPER_MODEL:-openai/whisper-large-v3-turbo}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-flash-latest}
      
      # Performance optimization settings
      - SKIP_EXISTING=${SKIP_EXISTING:-true}
      - OVERLAY_CHAPTER_TITLES=${OVERLAY_CHAPTER_TITLES:-true}
      - SKIP_DEPENDENCY_VALIDATION=${SKIP_DEPENDENCY_VALIDATION:-true}
      
      # AI Provider Configuration
      - AI_PROVIDER=${AI_PROVIDER:-local}
      - ENABLE_FALLBACK=${ENABLE_FALLBACK:-false}
      - LOCAL_MODEL_NAME=${LOCAL_MODEL_NAME:-phi4:latest}
      - LOCAL_MODEL_FRAMEWORK=${LOCAL_MODEL_FRAMEWORK:-ollama}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://localhost:11434}
      
      # Ollama Configuration
      - OLLAMA_MODELS=/workspace/models/ollama
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_CTX=16384
      
      # Development-friendly settings (Requirement 8.1, 8.5)
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEVELOPMENT_MODE=${DEVELOPMENT_MODE:-true}
      
      # Performance optimization for Docker on Windows
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
      - NUMBA_NUM_THREADS=8
      - TOKENIZERS_PARALLELISM=true
      - HF_HOME=/workspace/cache/huggingface
      
      # GPU environment variables (following NVIDIA Container Runtime docs)
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,video
    
    # Optimized resource allocation for Windows Docker performance
    # NVIDIA GPU configuration (using --gpus instead of runtime for better compatibility)
    deploy:
      resources:
        limits:
          memory: 20G
          cpus: '12.0'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
          memory: 6G
          cpus: '6.0'
    
    # Port mapping for Ollama API access
    ports:
      - "11434:11434"  # Ollama API port
    
    # Interactive development support (Requirements 8.1, 8.5)
    tty: true
    stdin_open: true
    
    # Performance optimizations for Windows Docker
    shm_size: '2gb'
    ulimits:
      memlock: -1
      stack: 67108864
    
    # Security options for better performance (bypass security overhead)
    security_opt:
      - seccomp:unconfined
    privileged: true
    
    # Default command for development
    command: ["./setup.sh"]