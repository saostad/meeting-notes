version: '3.8'

services:
  # CPU-only service for standard deployment (Requirements 5.1, 3.1)
  meeting-video-tool-cpu:
    # Use pre-built image from GHCR by default, or build locally if needed
    # To use GHCR image: set GHCR_USERNAME environment variable
    # To build locally: comment out the image line and uncomment build section
    image: ${GHCR_IMAGE_CPU:-meeting-video-tool:cpu}
    # Uncomment to build locally instead of using GHCR:
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    #   args:
    #     PYTORCH_INDEX_URL: https://download.pytorch.org/whl/cpu
    #     ENABLE_GPU: false
    container_name: meeting-video-tool-cpu
    
    # Volume mounts for input/output and model caching (Requirements 5.1, 5.2, 7.5)
    volumes:
      - ./videos:/input:ro                    # Input videos (read-only)
      - ./output:/output                      # Output files
      - model-cache:/cache                    # Persistent model cache
    
    # Environment configuration (Requirement 3.1)
    environment:
      # Required API key (Requirement 3.1)
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      
      # Model configuration (Requirement 7.2)
      - WHISPER_MODEL=${WHISPER_MODEL:-openai/whisper-large-v3-turbo}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-flash-latest}
      
      # Performance optimization settings (Requirements 7.3, 7.5)
      - SKIP_EXISTING=${SKIP_EXISTING:-false}
      - OUTPUT_DIR=/output
      - OVERLAY_CHAPTER_TITLES=${OVERLAY_CHAPTER_TITLES:-false}
      
      # Model caching configuration (Requirement 7.5)
      - HF_HOME=/cache/huggingface
      - TRANSFORMERS_CACHE=/cache/huggingface
      - HF_DATASETS_CACHE=/cache/huggingface/datasets
      - TORCH_HOME=/cache/models
    
    # Resource limits for optimal CPU performance
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    
    # Default command shows help
    command: ["python", "-m", "src.main", "--help"]

  # GPU-enabled service for accelerated processing (Requirement 6.1)
  meeting-video-tool-gpu:
    # Use pre-built image from GHCR by default, or build locally if needed
    # To use GHCR image: set GHCR_USERNAME environment variable
    # To build locally: comment out the image line and uncomment build section
    image: ${GHCR_IMAGE_GPU:-meeting-video-tool:gpu}
    # Uncomment to build locally instead of using GHCR:
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    #   args:
    #     PYTORCH_INDEX_URL: https://download.pytorch.org/whl/cu121
    #     ENABLE_GPU: true
    container_name: meeting-video-tool-gpu
    
    # Volume mounts identical to CPU version (Requirements 5.1, 5.2)
    volumes:
      - ./videos:/input:ro
      - ./output:/output
      - model-cache:/cache
    
    # Environment configuration with GPU-specific settings (Requirements 6.1, 3.1)
    environment:
      # Required API key
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      
      # Model configuration
      - WHISPER_MODEL=${WHISPER_MODEL:-openai/whisper-large-v3-turbo}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-flash-latest}
      
      # Performance optimization settings
      - SKIP_EXISTING=${SKIP_EXISTING:-false}
      - OUTPUT_DIR=/output
      - OVERLAY_CHAPTER_TITLES=${OVERLAY_CHAPTER_TITLES:-false}
      
      # Model caching configuration
      - HF_HOME=/cache/huggingface
      - TRANSFORMERS_CACHE=/cache/huggingface
      - HF_DATASETS_CACHE=/cache/huggingface/datasets
      - TORCH_HOME=/cache/models
      
      # GPU-specific environment variables (Requirement 6.1)
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    # GPU resource allocation and limits (Requirement 6.1)
    deploy:
      resources:
        limits:
          memory: 12G
          cpus: '6.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
          memory: 6G
          cpus: '3.0'
    
    # Default command shows help
    command: ["python", "-m", "src.main", "--help"]

# Named volumes for persistent model caching (Requirement 7.5)
volumes:
  model-cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/cache