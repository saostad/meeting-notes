# Gemini API Configuration
# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_api_key_here
GEMINI_MODEL=gemini-flash-latest

# Whisper Model Configuration
# Model will be downloaded automatically on first use
WHISPER_MODEL=openai/whisper-large-v3-turbo

# Output Configuration
# Directory where generated files will be saved (default: same as input file)
OUTPUT_DIR=./output

# Skip regenerating files that already exist
SKIP_EXISTING=false

# Video Overlay Configuration
# Overlay chapter titles on the video (top-right corner)
OVERLAY_CHAPTER_TITLES=false

# AI Provider Configuration
# Primary AI provider to use: "local" (Ollama) or "gemini"
AI_PROVIDER=local
# Use Gemini as fallback when primary provider fails
ENABLE_FALLBACK=false
# Local model name for Ollama
LOCAL_MODEL_NAME=phi4
# Local model framework: "ollama" or "auto"
LOCAL_MODEL_FRAMEWORK=ollama
# Ollama service URL
OLLAMA_BASE_URL=http://localhost:11434

# Performance Configuration
# Skip dependency validation for faster startup (Docker only)
SKIP_DEPENDENCY_VALIDATION=false
# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO
# Enable development mode features
DEVELOPMENT_MODE=false

# GPU Configuration (Docker only)
# Which GPU devices to use (0 for first GPU, all for all GPUs)
CUDA_VISIBLE_DEVICES=0

# Advanced Performance Settings
# Timeout for analysis operations in seconds
ANALYSIS_TIMEOUT=300
# Maximum memory usage in MB (empty for unlimited)
MAX_MEMORY_USAGE=
# Use GPU acceleration when available
USE_GPU=true
# JSON string for provider-specific model parameters
MODEL_PARAMETERS={}
