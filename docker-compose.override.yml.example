# Docker Compose override examples for different deployment scenarios
# Copy this file to docker-compose.override.yml and modify as needed
# 
# This file demonstrates various configuration patterns for different use cases

version: '3.8'

services:
  # =============================================================================
  # EXAMPLE 1: Co-located output (files saved alongside input videos)
  # =============================================================================
  # This configuration removes the :ro flag from input volume to make it writable
  # and doesn't set OUTPUT_DIR, so the container defaults to using input directory
  
  meeting-video-tool-cpu:
    volumes:
      - ./videos:/input                       # Writable input directory for co-located output
      - model-cache:/cache
    environment:
      # Don't set OUTPUT_DIR to use default behavior (input directory)
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - WHISPER_MODEL=${WHISPER_MODEL:-openai/whisper-large-v3-turbo}
      - SKIP_EXISTING=true                    # Skip files that already exist

  meeting-video-tool-gpu:
    volumes:
      - ./videos:/input                       # Writable input directory for co-located output
      - model-cache:/cache
    environment:
      # Don't set OUTPUT_DIR to use default behavior (input directory)
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - WHISPER_MODEL=${WHISPER_MODEL:-openai/whisper-large-v3-turbo}
      - SKIP_EXISTING=true

# =============================================================================
# EXAMPLE 2: Custom output directory with different models
# =============================================================================
# services:
#   meeting-video-tool-cpu:
#     volumes:
#       - ./videos:/input:ro
#       - ./processed:/custom-output
#       - model-cache:/cache
#     environment:
#       - OUTPUT_DIR=/custom-output
#       - WHISPER_MODEL=openai/whisper-base    # Faster, less accurate
#       - OVERLAY_CHAPTER_TITLES=true

# =============================================================================
# EXAMPLE 3: Multiple input directories for batch processing
# =============================================================================
# services:
#   meeting-video-tool-cpu:
#     volumes:
#       - ./meetings-2024:/input/2024:ro
#       - ./meetings-2025:/input/2025:ro
#       - ./output:/output
#       - model-cache:/cache
#     environment:
#       - OUTPUT_DIR=/output
#       - SKIP_EXISTING=true

# =============================================================================
# EXAMPLE 4: Network storage and custom cache location
# =============================================================================
# services:
#   meeting-video-tool-cpu:
#     volumes:
#       - /mnt/nas/videos:/input:ro
#       - /mnt/nas/processed:/output
#       - /fast-ssd/cache:/cache              # Use fast SSD for model cache
#     environment:
#       - OUTPUT_DIR=/output
#       - WHISPER_MODEL=openai/whisper-large-v3-turbo

# =============================================================================
# EXAMPLE 5: GPU-specific optimizations
# =============================================================================
# services:
#   meeting-video-tool-gpu:
#     volumes:
#       - ./videos:/input:ro
#       - ./output:/output
#       - model-cache:/cache
#     environment:
#       - CUDA_VISIBLE_DEVICES=1             # Use specific GPU
#       - WHISPER_MODEL=openai/whisper-large-v3-turbo
#       - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
#     deploy:
#       resources:
#         limits:
#           memory: 20G                       # Higher memory for large models
#         reservations:
#           devices:
#             - driver: nvidia
#               count: 1
#               capabilities: [gpu]

# =============================================================================
# EXAMPLE 6: Production deployment with resource limits
# =============================================================================
# services:
#   meeting-video-tool-cpu:
#     restart: unless-stopped
#     volumes:
#       - ./videos:/input:ro
#       - ./output:/output
#       - model-cache:/cache
#     environment:
#       - SKIP_EXISTING=true
#       - LOG_LEVEL=INFO
#     deploy:
#       resources:
#         limits:
#           memory: 6G
#           cpus: '2.0'
#         reservations:
#           memory: 3G
#           cpus: '1.0'
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"